{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28ea7ab6",
   "metadata": {},
   "source": [
    "# Memory-Efficient Text Embedding\n",
    "\n",
    "**Problem identified:**\n",
    "- Max text length: 7,041,805 characters (7M!)\n",
    "- GPU: 12.43 GB (but 10GB already in use)\n",
    "- Model is very large\n",
    "\n",
    "**Solutions:**\n",
    "1. Restart kernel to clear GPU memory\n",
    "2. Use CPU instead of GPU (slower but stable)\n",
    "3. Truncate texts to reasonable length (512-2048 tokens)\n",
    "4. Process with small batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "261c2b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dwright/miniforge3/envs/belief_model/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ef14013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DIAGNOSTIC: Check memory requirements before running full embedding\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import numpy as np\n",
    "\n",
    "# # Load data\n",
    "# input_path = \"../derived/user_text_and_stances.csv\"\n",
    "# model_name = \"Kingsoft-LLM/QZhou-Embedding\"\n",
    "\n",
    "# df = pd.read_csv(input_path)\n",
    "# raw_texts = df[\"all_text\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# print(f\"Total texts: {len(raw_texts)}\")\n",
    "# print(f\"\\nText length statistics (characters):\")\n",
    "# text_lengths = [len(t) for t in raw_texts]\n",
    "# print(f\"  Min: {min(text_lengths):,}\")\n",
    "# print(f\"  Max: {max(text_lengths):,}\")\n",
    "# print(f\"  Mean: {np.mean(text_lengths):,.0f}\")\n",
    "# print(f\"  Median: {np.median(text_lengths):,.0f}\")\n",
    "\n",
    "# # Check GPU memory if available\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "#     print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "#     print(f\"Currently allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "#     print(f\"Currently cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "# else:\n",
    "#     print(\"\\nNo GPU available - using CPU\")\n",
    "\n",
    "# # Load tokenizer to check token counts\n",
    "# print(\"\\nLoading tokenizer to check token lengths...\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# # Sample a few texts to check tokenized lengths\n",
    "# sample_indices = [0, len(raw_texts)//2, len(raw_texts)-1] if len(raw_texts) > 2 else [0]\n",
    "# print(\"\\nSample tokenized lengths:\")\n",
    "# for idx in sample_indices:\n",
    "#     tokens = tokenizer(raw_texts[idx], return_tensors=\"pt\", truncation=False)\n",
    "#     print(f\"  Text {idx}: {tokens['input_ids'].shape[1]:,} tokens\")\n",
    "\n",
    "# # Estimate memory for model\n",
    "# print(\"\\nLoading model to check memory footprint...\")\n",
    "# model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()\n",
    "#     torch.cuda.reset_peak_memory_stats(0)\n",
    "    \n",
    "#     # Test with one short text\n",
    "#     test_text = [\"This is a test.\"]\n",
    "#     test_input = tokenizer(test_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#     test_input = {k: v.to(device) for k, v in test_input.items()}\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         _ = model(**test_input)\n",
    "    \n",
    "#     peak_memory = torch.cuda.max_memory_allocated(0) / 1e9\n",
    "#     print(f\"\\nModel + short text (512 tokens) peak memory: {peak_memory:.2f} GB\")\n",
    "    \n",
    "#     # Estimate for max_length=8192\n",
    "#     estimated_8192 = peak_memory * (8192 / 512) * 1.5  # Rough scaling factor\n",
    "#     print(f\"Estimated memory for 8192 tokens: {estimated_8192:.2f} GB\")\n",
    "    \n",
    "#     available = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "#     print(f\"\\nAvailable GPU memory: {available:.2f} GB\")\n",
    "#     if estimated_8192 > available * 0.8:\n",
    "#         print(\"⚠️ WARNING: Texts may be too long for your GPU with max_length=8192\")\n",
    "#         suggested_max_len = int(512 * (available * 0.8 / peak_memory))\n",
    "#         print(f\"Suggested max_length: {suggested_max_len}\")\n",
    "\n",
    "# del model\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d728e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared\n",
      "GPU memory allocated: 0.00 GB\n",
      "GPU memory cached: 0.00 GB\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: Clear GPU memory from any stuck processes\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Clear Python's garbage collector\n",
    "gc.collect()\n",
    "\n",
    "# Clear PyTorch GPU cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "    print(\"GPU cache cleared\")\n",
    "    \n",
    "    # Check memory status\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "    \n",
    "    # If still full, you may need to restart the kernel\n",
    "    if torch.cuda.memory_reserved(0) / 1e9 > 1.0:\n",
    "        print(\"\\n⚠️ WARNING: GPU still has cached memory.\")\n",
    "        print(\"Consider: Kernel > Restart Kernel to fully clear GPU memory\")\n",
    "else:\n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b402d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7dd9d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 20314 texts.\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "input_path = \"../derived/user_text_and_stances.csv\"\n",
    "model_name = \"Kingsoft-LLM/QZhou-Embedding\"\n",
    "output_path = \"../derived/user_text_and_stances_qzhou_embedding_Kingsoft-LLM_QZhou-Embedding.csv\"\n",
    "\n",
    "# Settings - ADJUSTED FOR MEMORY CONSTRAINTS\n",
    "batch_size = 1\n",
    "max_length = 512  # Reduced from 8192 - this is critical for memory\n",
    "save_every_n_batches = 50  # Save more frequently\n",
    "use_cpu = True  # Set to False to try GPU (but may crash)\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(input_path)\n",
    "raw_texts = df[\"all_text\"].fillna(\"\").astype(str).tolist()\n",
    "user_ids = df[\"user_id\"].tolist()\n",
    "\n",
    "print(f\"Loaded {len(raw_texts)} texts.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e44cd99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Counting words: 100%|██████████| 20314/20314 [05:02<00:00, 67.17it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max text length: 1,375,556 words\n",
      "Average text length: 7056.1 words\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate text statistics using word tokenization\n",
    "word_counts = [len(word_tokenize(t)) for t in tqdm(raw_texts, desc=\"Counting words\")]\n",
    "max_words = max(word_counts)\n",
    "avg_words = sum(word_counts) / len(word_counts)\n",
    "print(f\"Max text length: {max_words:,} words\")\n",
    "print(f\"Average text length: {avg_words:.1f} words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "087dbb97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model Kingsoft-LLM/QZhou-Embedding...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU (slower but stable)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer and model\n",
    "print(f\"Loading model {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    padding_side=\"left\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Device selection\n",
    "if use_cpu:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU (slower but stable)\")\n",
    "else:\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def mean_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "    seq_lengths = attention_mask.sum(dim=-1)\n",
    "    return torch.stack(\n",
    "        [\n",
    "            last_hidden_states[i, -length:, :].sum(dim=0) / length\n",
    "            for i, length in enumerate(seq_lengths)\n",
    "        ],\n",
    "        dim=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42a0feb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting embedding process...\n",
      "Processing 20314 texts with batch_size=1, max_length=512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches:   0%|          | 5/20314 [00:54<61:07:11, 10.83s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Generate embeddings (use_cache=False to avoid compatibility issues)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 25\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m batch_embeds \u001b[38;5;241m=\u001b[39m mean_pool(outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state, batch_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     28\u001b[0m batch_embeds \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(batch_embeds, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/belief_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/belief_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Kingsoft_hyphen_LLM/QZhou_hyphen_Embedding/95b9d058e21b8d520bcbb1dd5df6765e3520ff5e/modeling_qzhou.py:802\u001b[0m, in \u001b[0;36mQZhouModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    791\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    792\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    793\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    799\u001b[0m         cache_position,\n\u001b[1;32m    800\u001b[0m     )\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 802\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbi_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    807\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    808\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    809\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    812\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniforge3/envs/belief_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/belief_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Kingsoft_hyphen_LLM/QZhou_hyphen_Embedding/95b9d058e21b8d520bcbb1dd5df6765e3520ff5e/modeling_qzhou.py:662\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    660\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    661\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 662\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    665\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniforge3/envs/belief_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/belief_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/Kingsoft_hyphen_LLM/QZhou_hyphen_Embedding/95b9d058e21b8d520bcbb1dd5df6765e3520ff5e/modeling_qzhou.py:234\u001b[0m, in \u001b[0;36mQwen2MLP.forward\u001b[0;34m(self, hidden_state)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_state):\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgate_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup_proj(hidden_state))\n",
      "File \u001b[0;32m~/miniforge3/envs/belief_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/belief_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniforge3/envs/belief_model/lib/python3.10/site-packages/torch/nn/modules/linear.py:134\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m    131\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;03m    Runs the forward pass.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Embed with incremental saving\n",
    "print(\"Starting embedding process...\")\n",
    "print(f\"Processing {len(raw_texts)} texts with batch_size={batch_size}, max_length={max_length}\")\n",
    "\n",
    "temp_embeddings = []\n",
    "temp_user_ids = []\n",
    "first_save = True\n",
    "\n",
    "for batch_idx, i in enumerate(tqdm(range(0, len(raw_texts), batch_size), desc=\"Embedding batches\")):\n",
    "    batch = raw_texts[i:i + batch_size]\n",
    "    batch_user_ids = user_ids[i:i + batch_size]\n",
    "    \n",
    "    # Tokenize with truncation\n",
    "    batch_dict = tokenizer(\n",
    "        batch,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n",
    "    \n",
    "    # Generate embeddings (use_cache=False to avoid compatibility issues)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch_dict, use_cache=False)\n",
    "    \n",
    "    batch_embeds = mean_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n",
    "    batch_embeds = F.normalize(batch_embeds, p=2, dim=1)\n",
    "    \n",
    "    # Store in temporary lists\n",
    "    temp_embeddings.extend(batch_embeds.cpu().tolist())\n",
    "    temp_user_ids.extend(batch_user_ids)\n",
    "    \n",
    "    # Clear cache\n",
    "    if device.type == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save incrementally\n",
    "    if (batch_idx + 1) % save_every_n_batches == 0 or i + batch_size >= len(raw_texts):\n",
    "        temp_df = pd.DataFrame({\n",
    "            \"user_id\": temp_user_ids,\n",
    "            \"qzhou_embedding\": temp_embeddings,\n",
    "        })\n",
    "        \n",
    "        temp_df.to_csv(\n",
    "            output_path, \n",
    "            mode='w' if first_save else 'a', \n",
    "            header=first_save, \n",
    "            index=False\n",
    "        )\n",
    "        \n",
    "        # Clear temporary storage\n",
    "        temp_embeddings = []\n",
    "        temp_user_ids = []\n",
    "        first_save = False\n",
    "        \n",
    "        if (batch_idx + 1) % (save_every_n_batches * 5) == 0:\n",
    "            print(f\"Progress: {min(i + batch_size, len(raw_texts))} / {len(raw_texts)} texts\")\n",
    "\n",
    "print(\"\\nEmbedding process completed.\")\n",
    "print(f\"Saved embeddings to {output_path}\")\n",
    "print(f\"\\nNote: Texts were truncated to {max_length} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c69dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DIAGNOSTIC: Check memory requirements before running full embedding\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import numpy as np\n",
    "\n",
    "# # Load data\n",
    "# input_path = \"../derived/user_text_and_stances.csv\"\n",
    "# model_name = \"Kingsoft-LLM/QZhou-Embedding\"\n",
    "\n",
    "# df = pd.read_csv(input_path)\n",
    "# raw_texts = df[\"all_text\"].fillna(\"\").astype(str).tolist()\n",
    "\n",
    "# print(f\"Total texts: {len(raw_texts)}\")\n",
    "# print(f\"\\nText length statistics (characters):\")\n",
    "# text_lengths = [len(t) for t in raw_texts]\n",
    "# print(f\"  Min: {min(text_lengths):,}\")\n",
    "# print(f\"  Max: {max(text_lengths):,}\")\n",
    "# print(f\"  Mean: {np.mean(text_lengths):,.0f}\")\n",
    "# print(f\"  Median: {np.median(text_lengths):,.0f}\")\n",
    "\n",
    "# # Check GPU memory if available\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"\\nGPU: {torch.cuda.get_device_name(0)}\")\n",
    "#     print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "#     print(f\"Currently allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "#     print(f\"Currently cached: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
    "# else:\n",
    "#     print(\"\\nNo GPU available - using CPU\")\n",
    "\n",
    "# # Load tokenizer to check token counts\n",
    "# print(\"\\nLoading tokenizer to check token lengths...\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# # Sample a few texts to check tokenized lengths\n",
    "# sample_indices = [0, len(raw_texts)//2, len(raw_texts)-1] if len(raw_texts) > 2 else [0]\n",
    "# print(\"\\nSample tokenized lengths:\")\n",
    "# for idx in sample_indices:\n",
    "#     tokens = tokenizer(raw_texts[idx], return_tensors=\"pt\", truncation=False)\n",
    "#     print(f\"  Text {idx}: {tokens['input_ids'].shape[1]:,} tokens\")\n",
    "\n",
    "# # Estimate memory for model\n",
    "# print(\"\\nLoading model to check memory footprint...\")\n",
    "# model = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()\n",
    "#     torch.cuda.reset_peak_memory_stats(0)\n",
    "    \n",
    "#     # Test with one short text\n",
    "#     test_text = [\"This is a test.\"]\n",
    "#     test_input = tokenizer(test_text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "#     test_input = {k: v.to(device) for k, v in test_input.items()}\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         _ = model(**test_input)\n",
    "    \n",
    "#     peak_memory = torch.cuda.max_memory_allocated(0) / 1e9\n",
    "#     print(f\"\\nModel + short text (512 tokens) peak memory: {peak_memory:.2f} GB\")\n",
    "    \n",
    "#     # Estimate for max_length=8192\n",
    "#     estimated_8192 = peak_memory * (8192 / 512) * 1.5  # Rough scaling factor\n",
    "#     print(f\"Estimated memory for 8192 tokens: {estimated_8192:.2f} GB\")\n",
    "    \n",
    "#     available = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "#     print(f\"\\nAvailable GPU memory: {available:.2f} GB\")\n",
    "#     if estimated_8192 > available * 0.8:\n",
    "#         print(\"⚠️ WARNING: Texts may be too long for your GPU with max_length=8192\")\n",
    "#         suggested_max_len = int(512 * (available * 0.8 / peak_memory))\n",
    "#         print(f\"Suggested max_length: {suggested_max_len}\")\n",
    "\n",
    "# del model\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415f0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # FIXED VERSION with realistic settings for your GPU\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# from tqdm.auto import tqdm\n",
    "# import gc\n",
    "\n",
    "# # Paths\n",
    "# input_path = \"../derived/user_text_and_stances.csv\"\n",
    "# model_name = \"Kingsoft-LLM/QZhou-Embedding\"\n",
    "# output_path = \"../derived/user_text_and_stances_qzhou_embedding_Kingsoft-LLM_QZhou-Embedding.csv\"\n",
    "\n",
    "# # Settings - ADJUSTED FOR YOUR GPU\n",
    "# batch_size = 1\n",
    "# max_length = 512  # Reduced from 8192 - this is critical!\n",
    "# save_every_n_batches = 50  # Save more frequently\n",
    "\n",
    "# print(\"IMPORTANT: Clearing GPU memory...\")\n",
    "# torch.cuda.empty_cache()\n",
    "# gc.collect()\n",
    "\n",
    "# # Load data\n",
    "# df = pd.read_csv(input_path)\n",
    "# raw_texts = df[\"all_text\"].fillna(\"\").astype(str).tolist()\n",
    "# user_ids = df[\"user_id\"].tolist()\n",
    "\n",
    "# print(f\"Loaded {len(raw_texts)} texts.\")\n",
    "# print(f\"Using max_length={max_length} tokens (texts will be truncated if longer)\")\n",
    "\n",
    "# # Tokenizer and model\n",
    "# print(\"Loading tokenizer...\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\n",
    "#     model_name,\n",
    "#     padding_side=\"left\",\n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "\n",
    "# print(\"Loading model... (this uses ~10GB GPU memory)\")\n",
    "# model = AutoModel.from_pretrained(\n",
    "#     model_name,\n",
    "#     trust_remote_code=True,\n",
    "#     torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "# )\n",
    "\n",
    "# device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# if torch.cuda.is_available():\n",
    "#     print(f\"GPU memory after loading model: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
    "\n",
    "# def mean_pool(last_hidden_states: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
    "#     seq_lengths = attention_mask.sum(dim=-1)\n",
    "#     return torch.stack(\n",
    "#         [\n",
    "#             last_hidden_states[i, -length:, :].sum(dim=0) / length\n",
    "#             for i, length in enumerate(seq_lengths)\n",
    "#         ],\n",
    "#         dim=0,\n",
    "#     )\n",
    "\n",
    "# # Embed with aggressive memory management\n",
    "# print(f\"Using device: {device}\")\n",
    "# print(\"Starting embedding process...\")\n",
    "\n",
    "# temp_embeddings = []\n",
    "# temp_user_ids = []\n",
    "# first_save = True\n",
    "# error_count = 0\n",
    "\n",
    "# for batch_idx, i in enumerate(tqdm(range(0, len(raw_texts), batch_size), desc=\"Embedding batches\")):\n",
    "#     try:\n",
    "#         batch = raw_texts[i:i + batch_size]\n",
    "#         batch_user_ids = user_ids[i:i + batch_size]\n",
    "        \n",
    "#         batch_dict = tokenizer(\n",
    "#             batch,\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=max_length,\n",
    "#             return_tensors=\"pt\",\n",
    "#         )\n",
    "#         batch_dict = {k: v.to(device) for k, v in batch_dict.items()}\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**batch_dict)\n",
    "        \n",
    "#         batch_embeds = mean_pool(outputs.last_hidden_state, batch_dict[\"attention_mask\"])\n",
    "#         batch_embeds = F.normalize(batch_embeds, p=2, dim=1)\n",
    "        \n",
    "#         # Move to CPU immediately\n",
    "#         temp_embeddings.extend(batch_embeds.cpu().tolist())\n",
    "#         temp_user_ids.extend(batch_user_ids)\n",
    "        \n",
    "#         # Aggressive cleanup\n",
    "#         del batch_dict, outputs, batch_embeds\n",
    "#         if device.type == \"cuda\":\n",
    "#             torch.cuda.empty_cache()\n",
    "        \n",
    "#         # Save incrementally\n",
    "#         if (batch_idx + 1) % save_every_n_batches == 0 or i + batch_size >= len(raw_texts):\n",
    "#             temp_df = pd.DataFrame({\n",
    "#                 \"user_id\": temp_user_ids,\n",
    "#                 \"qzhou_embedding\": temp_embeddings,\n",
    "#             })\n",
    "            \n",
    "#             temp_df.to_csv(\n",
    "#                 output_path, \n",
    "#                 mode='w' if first_save else 'a', \n",
    "#                 header=first_save, \n",
    "#                 index=False\n",
    "#             )\n",
    "            \n",
    "#             print(f\"✓ Saved checkpoint: {min(i + batch_size, len(raw_texts))} / {len(raw_texts)} texts\")\n",
    "            \n",
    "#             # Clear temporary storage\n",
    "#             temp_embeddings = []\n",
    "#             temp_user_ids = []\n",
    "#             first_save = False\n",
    "#             gc.collect()\n",
    "    \n",
    "#     except torch.cuda.OutOfMemoryError:\n",
    "#         error_count += 1\n",
    "#         print(f\"⚠️ OOM error at text {i}, skipping...\")\n",
    "#         if device.type == \"cuda\":\n",
    "#             torch.cuda.empty_cache()\n",
    "#         gc.collect()\n",
    "#         if error_count > 10:\n",
    "#             print(\"Too many errors, stopping.\")\n",
    "#             break\n",
    "\n",
    "# print(\"Embedding process completed.\")\n",
    "# print(f\"Saved embeddings to {output_path}\")\n",
    "# print(f\"Errors encountered: {error_count}\")\n",
    "\n",
    "# # Final cleanup\n",
    "# del model, tokenizer\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.empty_cache()\n",
    "# gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "belief_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
